{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import warnings\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "#from joblib import Parallel, delayed\n",
    "#import multiprocessing\n",
    "#from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import MinMaxScaler #, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, mean_absolute_percentage_error\n",
    "import math\n",
    "#from sklearn.model_selection import KFold\n",
    "#from sklearn.model_selection import TimeSeriesSplit\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection (Coleta de Dados) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import databases\n",
    "\n",
    "sales_df = pd.read_excel('C:\\\\Users\\\\sabrina\\\\Desktop\\\\Faturamento.xlsx') # Collecting data from an Excel file that contains sales history information since May-2019\n",
    "pd.set_option('display.max_columns', None)\n",
    "sales_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting rows with information not relevant to the analysis\n",
    "# The exclusion of these rows ensures that we have access only to information related to sales, disregarding possible returns, order refunds, and others.\n",
    "sales_df = sales_df.drop(sales_df[sales_df['TpDocVend.'] != 'Z000'].index)\n",
    "sales_df = sales_df.drop(sales_df[sales_df['G~Ctg.NF'] != '1N'].index)\n",
    "sales_df = sales_df.drop(sales_df[sales_df['G~Estornado'] == 'X'].index)\n",
    "sales_df = sales_df[sales_df['Material'] != '99000100'] # scrap\n",
    "\n",
    "sales_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA) (Análise Exploratória dos Dados) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General information about the dataframe\n",
    "sales_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there is any missing data\n",
    "sns.heatmap(sales_df.isnull(), yticklabels= False, cbar= False, cmap= 'Blues')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The heatmap shows us the correct information about the column 'G~Estornado'. This column should have all values empty, as it indicates information about invoice reversals.  \n",
    "Empty values in the column 'B~Tp.aval.' do not impact our analysis, as this column will be disregarded. This column refers to material characteristics and not to revenue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of a new column with the year from the column with the sales date information 'F~Dt.fatur.'\n",
    "sales_df['Year'] = pd.to_datetime(sales_df['F~Dt.fatur.']).dt.year\n",
    "\n",
    "# Group by year and sum the quantity of items already invoiced.\n",
    "yearly_sales = sales_df.groupby('Year')['Qtd.faturd'].sum()\n",
    "\n",
    "# Creating a bar chart for the aggregated data. This chart will show the evolution of the quantities of items sold over the years.\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(yearly_sales.index, yearly_sales.values)\n",
    "plt.title('The Annual Sum of the Quantity of Invoiced Items')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Invoiced Quantity')\n",
    "plt.xticks(rotation=45)  # Rotation of the x-axis labels for better visualization\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of a new column with the month from the column with the sales date information 'F~Dt.fatur.'\n",
    "sales_df['month'] = pd.to_datetime(sales_df['F~Dt.fatur.']).dt.month\n",
    "\n",
    "# Creation of the heatmap to visualize monthly sales data over the years\n",
    "heatmap_data = sales_df.pivot_table(values='Qtd.faturd', index='month', columns='Year', aggfunc='sum')\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(heatmap_data, annot=True, fmt=\".0f\", cmap='coolwarm')\n",
    "plt.title('Monthly Sales Heatmap by Year')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Month')\n",
    "plt.yticks(ticks=range(0, 12), labels=['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation (Preparação dos Dados) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to normalize the material data\n",
    "def normalize_material(df, column_name):\n",
    "    df[column_name] = df[column_name].astype(str).str.strip().str.upper()  # Converts to string, removes spaces, and converts to uppercase\n",
    "    df[column_name] = df[column_name].str.zfill(11)  # Pads with zeros on the left to ensure 11 digits\n",
    "    return df\n",
    "\n",
    "# Normalizing the specific column of each DataFrame\n",
    "sales_df = normalize_material(sales_df, 'Material')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of a new column with the year and month from the column with the sales date information 'F~Dt.fatur.'\n",
    "sales_df['YearMonth'] = pd.to_datetime(sales_df['F~Dt.fatur.']).dt.to_period('M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keeping only the columns necessary for the analysis\n",
    "'''\n",
    "Material - Column containing the part number information\n",
    "Qtd.faturd - Column containing the invoiced quantity per part number\n",
    "YearMonth - Column containing the invoicing month and year information\n",
    "'''\n",
    "sales_df = sales_df[['Material', 'Qtd.faturd', 'YearMonth']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot sales data by material and date, filled missing values\n",
    "sales_df = sales_df.pivot_table(index='Material', columns='YearMonth', values='Qtd.faturd', aggfunc='sum')\n",
    "sales_df = sales_df.fillna(0)\n",
    "sales_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove outliers from a row (data series) and replace with mean\n",
    "def remove_outliers_and_replace_with_mean(row):\n",
    "    Q1 = row.quantile(0.25)\n",
    "    Q3 = row.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    # Apply filter to find non-outlier values\n",
    "    valid_values = (row >= lower_bound) & (row <= upper_bound)\n",
    "    \n",
    "    # Calculate the mean of the non-outlier values\n",
    "    mean_valid_values = row[valid_values].mean()\n",
    "    \n",
    "    # Replace outliers with the calculated mean of valid values\n",
    "    row[~valid_values] = mean_valid_values\n",
    "    \n",
    "    return row\n",
    "\n",
    "# Applying the function remove_outliers_and_replace_with_mean on each row of the DataFrame\n",
    "sales_df = sales_df.apply(remove_outliers_and_replace_with_mean, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DataFrame sales_df now has outliers replaced by the mean of the non-outlier values for each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of months to be used for testing\n",
    "num_months_for_test = 12\n",
    "\n",
    "# Total number of month columns in the dataframe\n",
    "total_columns = sales_df.shape[1]\n",
    "\n",
    "# Calculation of columns for training\n",
    "num_columns_for_train = total_columns - num_months_for_test\n",
    "\n",
    "# Split dataset into training and test sets\n",
    "all_months = sales_df.columns[0:]\n",
    "training_set = sales_df[all_months[:-num_months_for_test]].fillna(0).values # Selects all columns representing months, except the last 12 (number defined in num_months_for_test)\n",
    "test_set = sales_df[all_months[-num_months_for_test:]].fillna(0).values # Selects only the last 12 month columns from the DataFrame, which correspond to the last 12 months of data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling features to range 0-1 for training and test sets\n",
    "sc = MinMaxScaler(feature_range=(0,1))\n",
    "\n",
    "training_set_scaled = sc.fit_transform(training_set)\n",
    "test_set_scaled = sc.fit_transform(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose Model (Escolha do Modelo) #    \n",
    "\n",
    "Here we chose the RNN - LSTM model and defined its architecture. LSTM is a good choice because it can capture long-term patterns in time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating empty lists\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "# Window size\n",
    "ws = 12 # months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing training data\n",
    "for material_index in range(training_set_scaled.shape[0]):  # Iterate over each row/material\n",
    "    for i in range(ws, num_columns_for_train - 1):  # Iterating from the beginning of the window to the second-to-last month available in the training set\n",
    "        X_train.append(training_set_scaled[material_index, i-ws:i]) # Each X_train[material_index] will have a sequence of ws months of data\n",
    "        y_train.append(training_set_scaled[material_index, i]) # Each y_train[material_index] will be the month following the ws window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xternal Loop (for material_index in range(training_set_scaled.shape[0])): Iterates over each row of the training_set_scaled array, where each row represents a different material.  \n",
    "\n",
    "Internal Loop (for i in range(ws, num_columns_for_train - 1)): For each material, this line of code is set up to iterate from the index ws to num_columns_for_train - 1. What this means is that you start iterating from the point where you have enough data to form your first data window (ws) and continue until the second-to-last month in the training set, which ensures that you always have future data available to use as y_train. \n",
    "\n",
    "X_train.append(...): Captures a window of ws months of data for the current material, starting at i-ws and ending at i (exclusive). \n",
    "\n",
    "y_train.append(...): Captures the data for the next immediate month after the ws-month window to use as the target variable for the model.  \n",
    "\n",
    "This setup is essential to ensure that the model learns to predict the following month based on a window of ws previous months for each individual material.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert lists to arrays for training the model\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of a RNN like LSTM in Keras, the input data must be three-dimensional with the dimensions being [number of samples, time steps, features per step].  \n",
    "This means we have 153,560 training examples, each with 12 time steps. Each time step consists of only one value, the invoiced amount, so X_train.shape = (153560, 12, 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training (Treinamento do Modelo) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, min_delta=0.0001, mode='min', verbose=1, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- monitor: Defines which metric the callback should monitor. Validation loss (val_loss) is common, but you can also monitor accuracy (val_accuracy) if relevant.  \n",
    "\n",
    "- patience: How many epochs to wait after the last improvement before stopping the training.  \n",
    "\n",
    "- min_delta: The minimum change in the monitored metric (in this case 'val_loss') to be considered an improvement. This basically means \"how much better the metric needs to be for it to be considered truly better.\"  \n",
    "\n",
    "- mode: Defines whether the training should stop when the monitored metric stops increasing (max) or stops decreasing (min).  \n",
    "\n",
    "- verbose: Displays log messages.  \n",
    "\n",
    "- restore_best_weights: If True, the model returns with the weights from the point where it achieved the best performance on the monitored metric. This is useful because the model may have overfitted after the best performance point before actually stopping training.  \n",
    "\n",
    "Adding Early Stopping not only helps prevent overfitting but also optimizes the use of computational resources by avoiding unnecessarily long training.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(units=60, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(LSTM(units=60, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(LSTM(units=60, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(LSTM(units=60))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(units=1))\n",
    "\n",
    "model.compile(optimizer= 'adam', loss= 'mean_squared_error')\n",
    "model.fit(X_train, y_train, epochs=100, batch_size= 32, validation_split=0.2, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the input_shape in the first LSTM layer is configured:     \n",
    "\n",
    "X_train.shape[1]: This refers to the second dimension of the X_train array, which in this case is the number of time steps per sequence (12). This value tells the model how many time steps each input sequence contains.  \n",
    "X_train.shape[2]: This refers to the third dimension of the X_train array, which would be the number of features at each time step. Since each time step contains only one value (the sales amount per month for a given material), this value is 1.    \n",
    "\n",
    "These indices ([1] and [2]) are used because the first index ([0]) represents the total number of sequences or samples, which is not part of the LSTM input_shape. The input_shape specifically needs to know the shape of each individual input sequence, excluding the dimension that counts the total number of sequences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the errors \n",
    "plt.plot(range(len(model.history.history['loss'])), model.history.history['loss'])\n",
    "plt.xlabel('Epoch Number')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions for all materials\n",
    "predictions_all_materials = []\n",
    "\n",
    "# Loop over each material\n",
    "for material_index in range(training_set_scaled.shape[0]):\n",
    "    batch_one = training_set_scaled[material_index, -ws:, np.newaxis] # Get the last window of data for the current material\n",
    "    batch_new = batch_one.reshape((1, ws, 1))\n",
    "    \n",
    "    prediction_test = []\n",
    "\n",
    "    # Predict the next 12 months\n",
    "    for i in range(12): \n",
    "        first_pred = model.predict(batch_new)[0]\n",
    "        prediction_test.append(first_pred)\n",
    "        batch_new = np.append(batch_new[:, 1:, :], [[first_pred]], axis=1) # Update the batch to include the new prediction\n",
    "    \n",
    "    # Reshape predictions to fit the scaler and inverse transform\n",
    "    prediction_test = np.array(prediction_test).reshape(1, -1)\n",
    "    predictions = sc.inverse_transform(prediction_test)\n",
    "    predictions_all_materials.append(predictions.flatten())\n",
    "\n",
    "# Convert list to array for easier handling\n",
    "predictions_all_materials = np.array(predictions_all_materials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) for material_index in range(training_set_scaled.shape[0]):  \n",
    "- training_set_scaled.shape[0] gets the number of rows in the scaled dataset, which corresponds to the number of materials.  \n",
    "- for material_index in range(...) iterates over each row, where each row represents a different material.   \n",
    "\n",
    "2) batch_one = training_set_scaled[material_index, -ws:, np.newaxis] & batch_new = batch_one.reshape((1, ws, 1)):  \n",
    "- training_set_scaled[material_index, -ws:, np.newaxis] selects the last ws (12) months of data for the current material, adding a new dimension to make it compatible with the input expected by the LSTM.  \n",
    "- .reshape((1, ws, 1)) reshapes the data to have the form (1, ws, 1), where 1 is the number of sequences (batches), ws is the number of time steps per sequence, and the last 1 is the number of features per time step.  \n",
    "\n",
    "3) for i in range(12):   \n",
    "    first_pred = model.predict(batch_new)[0]  \n",
    "    prediction_test.append(first_pred)  \n",
    "    batch_new = np.append(batch_new[:, 1:, :], [[first_pred]], axis=1):  \n",
    "\n",
    "- for i in range(12): iterates 12 times to generate predictions for the next 12 months.  \n",
    "- model.predict(batch_new)[0] makes a prediction using the current batch and takes the first element of the result (since the result is a batch, we need the first element).  \n",
    "- prediction_test.append(first_pred) adds the prediction to the prediction vector of the current material.  \n",
    "- np.append(...) updates the batch_new by removing the oldest month and adding the latest prediction at the end, keeping the batch size constant at 12 months.  \n",
    "\n",
    "4) prediction_test = np.array(prediction_test).reshape(1, -1)  \n",
    "predictions = sc.inverse_transform(prediction_test)  \n",
    "predictions_all_materials.append(predictions.flatten()):  \n",
    "- np.array(prediction_test).reshape(1, -1) converts the list of predictions into a NumPy array and reshapes it for compatibility with the inverse_transform method.  \n",
    "- sc.inverse_transform(prediction_test) applies the inverse transformation to convert the predictions back to the original data scale.  \n",
    "- predictions.flatten() flattens the resulting array to remove extra dimensions, and predictions_all_materials.append(...) adds the scaled predictions to the main vector that stores the predictions for all materials.   \n",
    "\n",
    "5) predictions_all_materials = np.array(predictions_all_materials):  \n",
    "- Converts the final list that contains the predictions for all materials into a NumPy array to facilitate subsequent handling of the data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the data for a single material\n",
    "actual = test_set[0, :]  # Adjust indices as needed\n",
    "predicted = predictions_all_materials[0, :]\n",
    "\n",
    "# Plot Predictions\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(actual, color='red', label='Actual Values')\n",
    "plt.plot(predicted, color='blue', label='Predicted Values')\n",
    "plt.title('Sales Forecast for Material X')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Invoiced Quantity')\n",
    "plt.xticks(ticks=np.arange(len(actual)), labels=[f'Month {i+1}' for i in range(len(actual))])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Model (Avaliação do Modelo) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lists to store the results\n",
    "rmse_list = []\n",
    "r_square_list = []\n",
    "material_ids = [f'Material_{i+1}' for i in range(predictions_all_materials.shape[0])]\n",
    "\n",
    "# Calculate metrics for each material\n",
    "for i in range(test_set.shape[0]):\n",
    "    rmse = math.sqrt(mean_squared_error(test_set[i], predictions_all_materials[i]))\n",
    "    r_square = r2_score(test_set[i], predictions_all_materials[i])\n",
    "    rmse_list.append(rmse)\n",
    "    r_square_list.append(r_square)\n",
    "    print(f'{material_ids[i]} - RMSE: {rmse}, R-Square: {r_square}')\n",
    "\n",
    "# Create a DataFrame to better visualize the results\n",
    "df_metrics = pd.DataFrame({\n",
    "    'Material': material_ids,\n",
    "    'RMSE': rmse_list,\n",
    "    'R-Square': r_square_list\n",
    "})\n",
    "\n",
    "print(df_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percetage_error_individual(y_true, y_pred):\n",
    "    mape_list = []\n",
    "    for i in range(y_true.shape[0]):  # Iterates over each row/material\n",
    "        true, pred = np.array(y_true[i]), np.array(y_pred[i])\n",
    "        mape = np.mean(np.abs((true - pred) / true)) * 100 if np.any(true) else np.nan  # Avoids division by zero\n",
    "        mape_list.append(mape)\n",
    "    return mape_list\n",
    "\n",
    "# Calculates the MAPE for each material\n",
    "mape_values = mean_absolute_percetage_error_individual(test_set, predictions_all_materials)\n",
    "material_ids = [f'Material_{i+1}' for i in range(predictions_all_materials.shape[0])]\n",
    "\n",
    "# Prints the MAPE for each material\n",
    "for material, mape in zip(material_ids, mape_values):\n",
    "    print(f'{material} - MAPE: {mape:.2f}%')\n",
    "\n",
    "# Create a DataFrame to better visualize the results\n",
    "df_mape = pd.DataFrame({\n",
    "    'Material': material_ids,\n",
    "    'MAPE': mape_values\n",
    "})\n",
    "\n",
    "print(df_mape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'predictions_all_materials' is a list of lists, where each sublist is a series of predictions for a material\n",
    "predictions_all_materials = np.array(predictions_all_materials)\n",
    "\n",
    "# Create a DataFrame from the numpy array\n",
    "# Assume you have a list or array of identifiers for each row (material)\n",
    "material_ids = [f'Material_{i+1}' for i in range(predictions_all_materials.shape[0])]\n",
    "\n",
    "# Create a DataFrame with column names representing future monthly periods (adjust as needed)\n",
    "future_months = [f'Month_{i+1}' for i in range(predictions_all_materials.shape[1])]\n",
    "df_predictions = pd.DataFrame(predictions_all_materials, index=material_ids, columns=future_months)\n",
    "\n",
    "# Export the DataFrame to an Excel file\n",
    "df_predictions.to_excel('predictions_all_materials.xlsx', index=True)\n",
    "\n",
    "# Display the created DataFrame\n",
    "df_predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
